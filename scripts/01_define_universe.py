import pandas as pd
import requests
import shutil
import sys
from pathlib import Path
from tqdm import tqdm
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì • (src.config ì„í¬íŠ¸ìš©)
BASE_DIR = Path(__file__).resolve().parent.parent
if str(BASE_DIR) not in sys.path:
    sys.path.insert(0, str(BASE_DIR))

from src.config import MASTER_PATH, BRONZE_DIR

# ì €ì¥ì†Œ ìœ„ì¹˜ ì •ì˜ (Script ì „ìš©)
# Yahoo í´ë”ëŠ” Configì˜ BRONZE_DIRê³¼ ë™ì¼
KAGGLE_DIR = BASE_DIR / "data" / "bronze" / "daily_prices" # Kaggle legacy

# SEC ë° NASDAQ ì†ŒìŠ¤
SEC_HEADERS = {'User-Agent': 'Individual_Researcher my_email@example.com'}
SEC_URL = "https://www.sec.gov/files/company_tickers.json"
NASDAQ_URL = "ftp://ftp.nasdaqtrader.com/SymbolDirectory/nasdaqtraded.txt"

def normalize_ticker(ticker):
    return str(ticker).strip().upper().replace(".", "-")

def get_new_tickers_from_web():
    """ì›¹ì—ì„œ ìµœì‹  ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘"""
    found_tickers = set()
    print("  ğŸ“¡ ì‹ ê·œ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì¤‘ (SEC/NASDAQ)...")
    try:
        resp = requests.get(SEC_URL, headers=SEC_HEADERS, timeout=5)
        if resp.status_code == 200:
            df = pd.DataFrame.from_dict(resp.json(), orient='index')
            found_tickers.update(df['ticker'].apply(normalize_ticker).tolist())
    except: pass

    try:
        df = pd.read_csv(NASDAQ_URL, sep="|")
        ts = df[df['Test Issue'] == 'N']['Symbol'].dropna().apply(normalize_ticker).tolist()
        found_tickers.update(ts)
    except: pass
        
    return found_tickers

def scan_and_resolve_conflicts():
    """Yahoo(ì‹ ê·œ)ì™€ Kaggle(êµ¬í˜•) ë°ì´í„° ì¶©ëŒ ìë™ í•´ê²°"""
    inventory = {} 
    
    # 1. Yahoo í´ë” ìŠ¤ìº” (ìš°ì„ ìˆœìœ„ 1ìœ„)
    if BRONZE_DIR.exists():
        for p in tqdm(list(BRONZE_DIR.glob("ticker=*")), desc="Scanning Yahoo"):
            if not p.is_dir(): continue
            ticker = p.name.split("=")[-1]
            ticker = normalize_ticker(ticker)
            
            file_path = p / "price.parquet"
            if file_path.exists():
                inventory[ticker] = {
                    'source': 'yahoo',
                    'file_path': str(file_path.relative_to(BASE_DIR)),
                    'is_active': True
                }

    # 2. Kaggle í´ë” ìŠ¤ìº” (ìš°ì„ ìˆœìœ„ 2ìœ„)
    if KAGGLE_DIR.exists():
        for p in tqdm(list(KAGGLE_DIR.glob("ticker=*")), desc="Scanning Kaggle"):
            if not p.is_dir(): continue
            original_ticker = p.name.split("=")[-1]
            original_ticker = normalize_ticker(original_ticker)
            
            file_path = p / "price.parquet"
            if not file_path.exists(): continue

            if original_ticker in inventory:
                new_ticker_name = f"{original_ticker}_OLD"
                inventory[new_ticker_name] = {
                    'source': 'kaggle_legacy',
                    'file_path': str(file_path.relative_to(BASE_DIR)),
                    'is_active': True
                }
            else:
                inventory[original_ticker] = {
                    'source': 'kaggle',
                    'file_path': str(file_path.relative_to(BASE_DIR)),
                    'is_active': True
                }
                
    return inventory

def main():
    print(">>> [Script 01] Universe ì •ì˜ ë° ì´ˆê¸°í™” (Pipeline í˜¸í™˜)")
    
    # 1. ë¡œì»¬ ìŠ¤ìº”
    local_data = scan_and_resolve_conflicts()
    print(f"  âœ… ë¡œì»¬ íŒŒì¼ ì‹ë³„: {len(local_data)} ê°œ")

    # 2. ê¸°ì¡´ ì¥ë¶€ ë°±ì—…
    old_meta = {}
    if MASTER_PATH.exists():
        backup = MASTER_PATH.parent / "master_ticker_list_backup.csv"
        shutil.copy2(MASTER_PATH, backup)
        df_old = pd.read_csv(MASTER_PATH)
        for _, row in df_old.iterrows():
            old_meta[row['ticker']] = row.to_dict()

    # 3. ë°ì´í„° í†µí•©
    final_rows = []
    today = datetime.now().strftime("%Y-%m-%d")
    
    # [A] ë¡œì»¬ íŒŒì¼
    for ticker, info in local_data.items():
        row = {
            'ticker': ticker,
            'source': info['source'],
            'is_active': info['is_active'],
            'file_path': info['file_path'],
            'count': 0,
            'start_date': None,
            'end_date': None,
            'last_updated': today,
            # [Pipeline í˜¸í™˜ í•„ë“œ ì¶”ê°€]
            'fail_count': 0,
            'last_failed_date': None,
            'note': 'Initialized from Local'
        }
        
        # ë©”íƒ€ë°ì´í„° ë³µì›
        if ticker in old_meta:
            prev = old_meta[ticker]
            if str(prev.get('file_path')) == str(info['file_path']):
                row.update({k: v for k, v in prev.items() if k in row})

        final_rows.append(row)

    # [B] ì›¹ ì‹ ê·œ
    web_tickers = get_new_tickers_from_web()
    existing = set(local_data.keys())
    
    cnt_new = 0
    for t in web_tickers:
        if t not in existing and f"{t}_OLD" not in existing:
            if "$" in t: continue
            final_rows.append({
                'ticker': t,
                'source': 'new_ipo',
                'is_active': True,
                'file_path': f"data/bronze/yahoo_price_data/ticker={t}/price.parquet",
                'count': 0,
                'start_date': None,
                'end_date': None,
                'last_updated': None, # ë‹¤ìš´ë¡œë“œ ëŒ€ìƒ ë§ˆí‚¹
                'fail_count': 0,
                'last_failed_date': None,
                'note': 'Discovered from Web'
            })
            cnt_new += 1
            
    print(f"  ğŸ” ì›¹ ì‹ ê·œ ì¶”ê°€: {cnt_new} ê°œ")

    # 4. ì €ì¥ ë° íŒŒì¼ ë©”íƒ€ë°ì´í„° ê°±ì‹ 
    df_final = pd.DataFrame(final_rows)
    
    print("  ğŸ“ ë©”íƒ€ë°ì´í„°(í–‰ ê°œìˆ˜) ê°±ì‹  ì¤‘...")
    for idx, row in tqdm(df_final.iterrows(), total=len(df_final)):
        if row['count'] == 0 and row['source'] != 'new_ipo':
            full_path = BASE_DIR / str(row['file_path'])
            if full_path.exists():
                try:
                    meta = pd.read_parquet(full_path, columns=['Close'])
                    df_final.at[idx, 'count'] = len(meta)
                    df_final.at[idx, 'start_date'] = meta.index[0].strftime("%Y-%m-%d")
                    df_final.at[idx, 'end_date'] = meta.index[-1].strftime("%Y-%m-%d")
                except: pass

    df_final.to_csv(MASTER_PATH, index=False)
    print("  âœ… ì´ˆê¸°í™” ì™„ë£Œ. (Master List Saved)")

if __name__ == "__main__":
    main()