import pandas as pd
import sys
from pathlib import Path
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).resolve().parent.parent
if str(BASE_DIR) not in sys.path:
    sys.path.insert(0, str(BASE_DIR))

from src.config import PLATINUM_FEATURES_DIR

# ì ê²€í•  ì£¼ìš” ëŒ€ì¥ì£¼ ë¦¬ìŠ¤íŠ¸ (Top 20 Mega Caps)
TARGET_GIANTS = [
    "AAPL", "MSFT", "GOOGL", "AMZN", "NVDA", "TSLA", "META", "BRK-B", "LLY", "V", 
    "TSM", "JPM", "WMT", "XOM", "UNH", "MA", "PG", "JNJ", "HD", "COST"
]

def diagnose_dataset():
    print(f"ğŸ” [ë°ì´í„°ì…‹ ì •ë°€ ì§„ë‹¨] Platinum ë°ì´í„°ë¥¼ ê²€ì‚¬í•©ë‹ˆë‹¤...")
    print(f"ğŸ“‚ ê²½ë¡œ: {PLATINUM_FEATURES_DIR}\n")

    if not PLATINUM_FEATURES_DIR.exists():
        print("âŒ Platinum ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 1. íŒŒì¼ ëª©ë¡ ë¡œë“œ
    files = list(PLATINUM_FEATURES_DIR.glob("*.parquet"))
    existing_tickers = set(f.stem for f in files)
    
    print(f"âœ… ì´ ë°œê²¬ëœ íŒŒì¼ ìˆ˜: {len(files)}ê°œ")

    # ==========================================
    # [ì§„ë‹¨ 1] ì£¼ìš” ëŒ€ì¥ì£¼(Giants) ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    # ==========================================
    print("\n" + "="*50)
    print("ğŸ’ [Step 1] ì£¼ìš” ëŒ€ì¥ì£¼ ì¡´ì¬ ì—¬ë¶€ ì²´í¬")
    print("="*50)
    
    missing_giants = []
    found_giants = []
    
    for t in TARGET_GIANTS:
        if t in existing_tickers:
            found_giants.append(t)
        else:
            missing_giants.append(t)
            
    if found_giants:
        print(f"âœ… ë°œê²¬ë¨ ({len(found_giants)}ê°œ): {found_giants}")
    
    if missing_giants:
        print(f"ğŸš¨ [CRITICAL] ëˆ„ë½ë¨ ({len(missing_giants)}ê°œ): {missing_giants}")
        print("   -> 01_define_universe.py ë˜ëŠ” 02_data_download.pyì—ì„œ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("   -> 'scripts/force_download_giants.py'ë¥¼ ì‹¤í–‰í•˜ì—¬ ê¸´ê¸‰ ë³µêµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        print("ğŸ‰ ëª¨ë“  ëŒ€ì¥ì£¼ê°€ ì •ìƒì ìœ¼ë¡œ ì¡´ì¬í•©ë‹ˆë‹¤.")

    # ==========================================
    # [ì§„ë‹¨ 2] ì¡ì£¼(XYZ) ë° ì´ìƒ ì¢…ëª© í™•ì¸
    # ==========================================
    print("\n" + "="*50)
    print("ğŸ—‘ï¸ [Step 2] í…ŒìŠ¤íŠ¸ìš© ì¡ì£¼(XYZ) í™•ì¸")
    print("="*50)
    
    suspicious = ["XYZ", "ABC", "TEST"]
    found_suspicious = [t for t in suspicious if t in existing_tickers]
    
    if found_suspicious:
        print(f"âš ï¸ [WARNING] í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë°œê²¬: {found_suspicious}")
        print("   -> ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì™œê³¡í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‚­ì œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
    else:
        print("âœ… ì´ìƒí•œ ì¢…ëª©(XYZ ë“±)ì€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # ==========================================
    # [ì§„ë‹¨ 3] FD ë³€ìˆ˜ ê±´ê°• ìƒíƒœ ì²´í¬ (ê¸°ì¡´ ë¡œì§)
    # ==========================================
    print("\n" + "="*50)
    print("ğŸ¥ [Step 3] FD ë³€ìˆ˜ ê²°ì¸¡(NaN) ì§„ë‹¨")
    print("="*50)
    
    missing_fd_files = []
    all_nan_files = []
    
    # ë„ˆë¬´ ë§ìœ¼ë©´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ Giants ìœ„ì£¼ë¡œ ë¨¼ì € ìƒ˜í”Œë§í•˜ê±°ë‚˜ ì „ì²´ ìˆ˜í–‰
    # ì—¬ê¸°ì„œëŠ” ë°œê²¬ëœ Giants + ëœë¤ 100ê°œë§Œ ì²´í¬
    check_targets = list(found_giants) + list(existing_tickers)[:100]
    check_targets = list(set(check_targets)) # ì¤‘ë³µ ì œê±°
    
    for t in tqdm(check_targets, desc="Checking FD Columns"):
        file_path = PLATINUM_FEATURES_DIR / f"{t}.parquet"
        try:
            df = pd.read_parquet(file_path)
            fd_cols = [c for c in df.columns if c.startswith('FD_')]
            
            if not fd_cols:
                missing_fd_files.append(t)
                continue
            
            # ì²« ë²ˆì§¸ FD ì»¬ëŸ¼ ê¸°ì¤€ ê²€ì‚¬
            target_col = 'FD_Close' if 'FD_Close' in fd_cols else fd_cols[0]
            if df[target_col].isna().all():
                all_nan_files.append(t)
                
        except Exception:
            pass

    if missing_fd_files:
        print(f"\nâŒ FD ì»¬ëŸ¼ ì—†ìŒ: {len(missing_fd_files)}ê°œ ({missing_fd_files[:5]}...)")
    if all_nan_files:
        print(f"ğŸ’€ FD ì „ë¶€ NaN (ê³„ì‚°ì‹¤íŒ¨): {len(all_nan_files)}ê°œ ({all_nan_files[:5]}...)")
    
    if not missing_fd_files and not all_nan_files:
        print("âœ… ì²´í¬í•œ íŒŒì¼ë“¤ì˜ ë°ì´í„° ìƒíƒœëŠ” ì–‘í˜¸í•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    diagnose_dataset()