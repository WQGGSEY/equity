import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
from joblib import Parallel, delayed

# ==========================================
# [ì„¤ì •]
# ==========================================
BASE_DIR = Path(__file__).resolve().parent.parent
MASTER_PATH = BASE_DIR / "data" / "bronze" / "master_ticker_list.csv"
KAGGLE_DIR = BASE_DIR / "data" / "bronze" / "daily_prices"
REF_DIR = BASE_DIR / "data" / "temp_reference"
OUTPUT_PLAN = BASE_DIR / "data" / "bronze" / "fix_plan.json"

# ë³‘ë ¬ ì²˜ë¦¬ ê°œìˆ˜ (M2 ì„±ëŠ¥ í™œìš©)
N_JOBS = 6

# [ê°•í™”ëœ ë§¤ì¹­ ì„¤ì • - User Suggestion: 20 Days]
MIN_OVERLAP_DAYS = 30       
CORRELATION_THRESHOLD = 0.99 

# ì§€ë¬¸ ëŒ€ì¡°ìš© ì„¤ì •
FINGERPRINT_DAYS = 20       # 20ì¼(ì•½ 1ë‹¬) íŒ¨í„´ ë¹„êµ -> ì˜¤íƒ í™•ë¥  0% ìˆ˜ë ´
SEQUENCE_CORR_THRESHOLD = 0.999 # íŒ¨í„´ ì¼ì¹˜ë„ 99.9% ìš”êµ¬
PRICE_TOLERANCE = 0.05      # ê°€ê²© ì˜¤ì°¨ 5% í—ˆìš©

def load_master():
    if not MASTER_PATH.exists():
        raise FileNotFoundError("Master list not found!")
    df = pd.read_csv(MASTER_PATH)
    if 'start_date' in df.columns:
        df['start_date'] = pd.to_datetime(df['start_date'])
    if 'end_date' in df.columns:
        df['end_date'] = pd.to_datetime(df['end_date'])
    return df

def load_parquet(path):
    try:
        df = pd.read_parquet(path)
        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'])
            df = df.set_index('Date').sort_index()
        return df
    except:
        return None

def analyze_ticker(row, ref_tickers_set):
    """1ì°¨ ë¶„ì„: ì´ë¦„ ì¶©ëŒ ë° ì§€ë¬¸ ì¶”ì¶œ"""
    ticker = row['ticker']
    kaggle_path = Path(row['file_path'])
    if not kaggle_path.is_absolute():
        kaggle_path = BASE_DIR / kaggle_path
        
    kaggle_df = load_parquet(kaggle_path)
    if kaggle_df is None or len(kaggle_df) < 10:
        return None 

    # CASE 1: ì´ë¦„ ì¶©ëŒ (Direct Match)
    if ticker in ref_tickers_set:
        ref_path = REF_DIR / f"ticker={ticker}" / "price.parquet"
        ref_df = load_parquet(ref_path)
        
        if ref_df is not None:
            common_idx = kaggle_df.index.intersection(ref_df.index)
            
            if len(common_idx) > MIN_OVERLAP_DAYS:
                k_close = kaggle_df.loc[common_idx, 'Close']
                r_close = ref_df.loc[common_idx, 'Close']
                
                # Zero Variance Check
                if k_close.std() == 0 or r_close.std() == 0:
                    return {"ticker": ticker, "action": "FORK", "reason": "Zero Variance", "new_name": f"{ticker}_legacy"}

                k_ret = k_close.pct_change().dropna()
                r_ret = r_close.pct_change().dropna()
                
                if len(k_ret) < 2 or k_ret.std() == 0 or r_ret.std() == 0:
                     return {"ticker": ticker, "action": "FORK", "reason": "Bad Data for Corr", "new_name": f"{ticker}_legacy"}
                
                corr = k_ret.corr(r_ret)
                
                if pd.isna(corr):
                     return {"ticker": ticker, "action": "FORK", "reason": "NaN Correlation", "new_name": f"{ticker}_legacy"}

                if corr > CORRELATION_THRESHOLD:
                    return {"ticker": ticker, "action": "MERGE", "reason": f"High Correlation ({corr:.4f})", "target_path": str(ref_path)}
                else:
                    return {"ticker": ticker, "action": "FORK", "reason": f"Low Correlation ({corr:.4f})", "new_name": f"{ticker}_legacy"}
            else:
                gap_days = (ref_df.index[0] - kaggle_df.index[-1]).days
                if gap_days > 60:
                    return {"ticker": ticker, "action": "FORK", "reason": f"Gap {gap_days} days", "new_name": f"{ticker}_legacy"}
                else:
                    return {"ticker": ticker, "action": "MERGE", "reason": "Sequential Data", "target_path": str(ref_path)}

    # CASE 2: ì§€ë¬¸ ì¶”ì¶œ (ìµœëŒ€ 20ì¼)
    # ë°ì´í„°ê°€ 20ì¼ë³´ë‹¤ ì ìœ¼ë©´ ìˆëŠ” ë§Œí¼ë§Œ ì‚¬ìš©
    lookback = min(len(kaggle_df), FINGERPRINT_DAYS)
    last_seq = kaggle_df.iloc[-lookback:]
    
    fingerprint = {
        "dates": [d.strftime('%Y-%m-%d') for d in last_seq.index],
        "prices": last_seq['Close'].tolist(),
        "ticker": ticker
    }
    
    return {
        "ticker": ticker,
        "action": "SEARCH_CANDIDATE",
        "fingerprint": fingerprint
    }

def find_alias_batch_v3(candidates, ref_tickers):
    """
    [ê°œì„ ëœ 2ì°¨ ìˆ˜ìƒ‰ëŒ€]
    - 20ì¼ì¹˜ ê°€ê²© ì‹œí€€ìŠ¤ë¥¼ ë¹„êµí•˜ì—¬ Unique Matchë¥¼ ì°¾ìŒ
    """
    print(f"\n>>> ğŸ•µï¸ 20ì¼ì¹˜ ì •ë°€ íŒ¨í„´ ëŒ€ì¡° ì¤‘... (ëŒ€ìƒ: {len(candidates)}ê°œ ì¢…ëª©)")
    
    # ë‚ ì§œë³„ Lookup Table êµ¬ì„± (Key: ë§ˆì§€ë§‰ ë‚ ì§œ)
    lookup_table = {}
    for c in candidates:
        fp = c['fingerprint']
        last_date = fp['dates'][-1]
        
        if last_date not in lookup_table:
            lookup_table[last_date] = []
        lookup_table[last_date].append(c)

    ref_files = list(REF_DIR.glob("ticker=*/price.parquet"))
    
    def scan_ref_file(ref_file):
        matches_found = []
        try:
            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë¡œë“œ
            df = pd.read_parquet(ref_file, columns=['Date', 'Close'])
            # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ë¹„êµ ë¶ˆê°€ (ìµœì†Œ 3ì¼ì€ ìˆì–´ì•¼ í•¨)
            if df.empty or len(df) < 3: return []
            
            df['date_str'] = df['Date'].dt.strftime('%Y-%m-%d')
            ref_ticker = ref_file.parent.name.replace("ticker=", "")
            
            # êµì§‘í•© ë‚ ì§œ í™•ì¸
            available_dates = set(df['date_str'])
            target_dates = set(lookup_table.keys())
            common_last_dates = available_dates.intersection(target_dates)
            
            for l_date in common_last_dates:
                # l_date ìœ„ì¹˜ ì°¾ê¸°
                curr_rows = df[df['date_str'] == l_date]
                if curr_rows.empty: continue
                curr_idx = curr_rows.index[0] # RangeIndex ê°€ì •
                
                loc_idx = df.index.get_loc(curr_idx)
                
                for candidate in lookup_table[l_date]:
                    cand_prices = candidate['fingerprint']['prices']
                    seq_len = len(cand_prices)
                    
                    # Yahoo ë°ì´í„° ë²”ìœ„ ì²´í¬
                    if loc_idx < seq_len - 1:
                        continue 
                        
                    # Yahoo ì‹œí€€ìŠ¤ ì¶”ì¶œ
                    ref_seq = df['Close'].iloc[loc_idx - (seq_len - 1) : loc_idx + 1].tolist()
                    
                    # A. ë§ˆì§€ë§‰ ë‚  ê°€ê²© ì˜¤ì°¨ ê²€ì‚¬
                    p_cand_last = cand_prices[-1]
                    p_ref_last = ref_seq[-1]
                    
                    if abs(p_ref_last - p_cand_last) / p_cand_last > PRICE_TOLERANCE:
                        continue
                        
                    # B. íŒ¨í„´ ë§¤ì¹­ (MSE & Correlation)
                    score = 0
                    if seq_len >= 5: # 5ì¼ ì´ìƒì¼ ë•Œë§Œ ìƒê´€ê³„ìˆ˜ ì‹ ë¢°
                        cand_norm = np.array(cand_prices) / cand_prices[0]
                        ref_norm = np.array(ref_seq) / ref_seq[0]
                        
                        mse = np.mean((cand_norm - ref_norm) ** 2)
                        
                        if np.std(cand_norm) > 0 and np.std(ref_norm) > 0:
                            corr = np.corrcoef(cand_norm, ref_norm)[0, 1]
                            if corr < SEQUENCE_CORR_THRESHOLD: 
                                continue
                        
                        score = mse
                    else:
                        # ë°ì´í„° ì§§ìœ¼ë©´ ë‹¨ìˆœ ì˜¤ì°¨ ì‚¬ìš©
                        score = abs(p_ref_last - p_cand_last) / p_cand_last

                    matches_found.append({
                        "k_ticker": candidate['ticker'],
                        "y_ticker": ref_ticker,
                        "score": score,
                        "reason": f"Seq Match (len={seq_len}, score={score:.6f})"
                    })
                    
            return matches_found

        except Exception:
            return []

    # ë³‘ë ¬ ìŠ¤ìº”
    scan_results = Parallel(n_jobs=N_JOBS)(
        delayed(scan_ref_file)(f) for f in tqdm(ref_files, desc="Scanning Reference Universe")
    )
    
    # Best Match Selection (Winner Takes All)
    best_matches = {} 
    
    for batch in scan_results:
        for m in batch:
            k = m['k_ticker']
            if k not in best_matches:
                best_matches[k] = m
            else:
                if m['score'] < best_matches[k]['score']:
                    best_matches[k] = m
                    
    return best_matches

def main():
    print(">>> [Phase 2: V3.1 Final] 20-Day Sequence Fingerprinting")
    
    master = load_master()
    ref_tickers = {p.name.replace("ticker=", "") for p in REF_DIR.glob("ticker=*")}
    
    print(">>> 1ì°¨ ë¶„ì„: ì´ë¦„ì´ ê°™ì€ ì¢…ëª© ê²€ì¦ ì¤‘...")
    results = Parallel(n_jobs=N_JOBS)(
        delayed(analyze_ticker)(row, ref_tickers) 
        for _, row in tqdm(master.iterrows(), total=len(master))
    )
    results = [r for r in results if r is not None]
    
    plan = []
    search_candidates = []
    
    for res in results:
        if res['action'] == 'SEARCH_CANDIDATE':
            search_candidates.append(res)
        else:
            plan.append(res)
            
    # 2ì°¨ ë¶„ì„
    if search_candidates:
        best_matches = find_alias_batch_v3(search_candidates, ref_tickers)
        
        matched_set = set()
        for k_ticker, info in best_matches.items():
            plan.append({
                "ticker": k_ticker,
                "action": "RENAME",
                "new_name": info['y_ticker'],
                "reason": info['reason']
            })
            matched_set.add(k_ticker)
            
        for c in search_candidates:
            if c['ticker'] not in matched_set:
                plan.append({
                    "ticker": c['ticker'],
                    "action": "MISSING",
                    "reason": "No sequence match found"
                })

    with open(OUTPUT_PLAN, 'w') as f:
        json.dump(plan, f, indent=4)
        
    df_plan = pd.DataFrame(plan)
    print("\n" + "="*40)
    print(f"  ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {OUTPUT_PLAN}")
    print("="*40)
    print(df_plan['action'].value_counts())
    
    print("\n[RENAME ì œì•ˆ ì˜ˆì‹œ - ìƒìœ„ 5ê°œ]")
    renames = df_plan[df_plan['action'] == 'RENAME']
    if not renames.empty:
        print(renames.head(5)[['ticker', 'new_name', 'reason']].to_string(index=False))
    else:
        print("  (RENAME ì œì•ˆ ì—†ìŒ)")

if __name__ == "__main__":
    main()