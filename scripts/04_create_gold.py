import pandas as pd
import shutil
import numpy as np
from pathlib import Path
from tqdm import tqdm
from collections import defaultdict
import warnings

# ê²½ê³  ë¬´ì‹œ (ìƒê´€ê³„ìˆ˜ ê³„ì‚° ì‹œ runtime warning ë“±)
warnings.simplefilter(action='ignore', category=FutureWarning)

# ==========================================
# [Phase 4] Gold Layer: Robust Dedup (Corr & Stitch)
# ==========================================
BASE_DIR = Path(__file__).resolve().parent.parent
SILVER_DIR = BASE_DIR / "data" / "silver" / "daily_prices"
GOLD_DIR = BASE_DIR / "data" / "gold" / "daily_prices"

def get_metadata(file_path):
    """
    íŒŒì¼ì„ ê°€ë³ê²Œ ì½ì–´ì„œ 'ì‹œì‘ì¼(Start Date)'ê³¼ 'ì¢…ë£Œì¼(End Date)' ì¶”ì¶œ
    ì´ ì •ë³´ë¡œ 1ì°¨ ê·¸ë£¹í•‘ì„ ìˆ˜í–‰í•¨ (ê°€ê²© ë¹„êµ X)
    """
    try:
        # ì¸ë±ìŠ¤ë§Œ ë¹ ë¥´ê²Œ ë¡œë“œ ê°€ëŠ¥í•˜ë©´ ì¢‹ì§€ë§Œ, parquet íŠ¹ì„±ìƒ ì»¬ëŸ¼ í•˜ë‚˜ ì½ëŠ”ê²Œ ë¹ ë¦„
        df = pd.read_parquet(file_path, columns=['Close'])
        if df.empty: return None
        
        start_date = df.index[0]
        end_date = df.index[-1]
        
        # ê·¸ë£¹í•‘ í‚¤: "YYYY-MM" (ê°™ì€ ë‹¬ì— ì‹œì‘í•œ ì¢…ëª©ë¼ë¦¬ ë¹„êµ)
        start_key = start_date.strftime("%Y-%m")
        
        return {
            'ticker': file_path.stem,
            'path': file_path,
            'start_key': start_key,
            'start_date': start_date,
            'end_date': end_date,
            'count': len(df)
        }
    except:
        return None

def calculate_correlation(path_a, path_b):
    """
    [Robust] ë‘ íŒŒì¼ì˜ ê²¹ì¹˜ëŠ” êµ¬ê°„ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
    - í‘œì¤€í¸ì°¨ê°€ 0ì¸(ì£¼ê°€ ë³€ë™ ì—†ëŠ”) ê²½ìš°ë¥¼ ë°©ì–´í•˜ì—¬ RuntimeWarning ì œê±°
    """
    try:
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë¡œë“œ
        df_a = pd.read_parquet(path_a, columns=['Close'])
        df_b = pd.read_parquet(path_b, columns=['Close'])
        
        # êµì§‘í•© êµ¬ê°„ ì°¾ê¸°
        common_idx = df_a.index.intersection(df_b.index)
        
        # ê²¹ì¹˜ëŠ” êµ¬ê°„ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ íŒë‹¨ ë¶ˆê°€ (ìµœì†Œ 30ì¼)
        if len(common_idx) < 30:
            return 0.0
            
        series_a = df_a.loc[common_idx, 'Close'].astype(float)
        series_b = df_b.loc[common_idx, 'Close'].astype(float)
        
        # [í•µì‹¬ ìˆ˜ì •] í‘œì¤€í¸ì°¨ê°€ 0ì¸ì§€ í™•ì¸ (Constant Value Check)
        # 1e-9ë³´ë‹¤ ì‘ìœ¼ë©´ ë³€ë™ì´ ê±°ì˜ ì—†ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼
        if series_a.std() < 1e-9 or series_b.std() < 1e-9:
            return 0.0 # ë³€ë™ì´ ì—†ìœ¼ë©´ ìƒê´€ê´€ê³„ ê³„ì‚° ë¶ˆê°€ -> ë¬´ì‹œ
            
        # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        corr = series_a.corr(series_b)
        
        # ê²°ê³¼ê°€ NaNì´ë©´ 0ìœ¼ë¡œ ì²˜ë¦¬
        if pd.isna(corr):
            return 0.0
            
        return corr
    except Exception:
        # íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ë“± ëª¨ë“  ì—ëŸ¬ ì‹œ 0.0 ë°˜í™˜ (ì•ˆì „í•˜ê²Œ Skip)
        return 0.0

def stitch_and_save(main_meta, sub_metas, output_dir):
    """
    Main ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ Sub ë°ì´í„°ë“¤ì„ ì´ì–´ë¶™ì—¬ì„œ(Stitching) ì €ì¥
    """
    try:
        # 1. Main ë¡œë“œ
        main_df = pd.read_parquet(main_meta['path'])
        
        # 2. Sub ìˆœíšŒí•˜ë©° êµ¬ë© ë©”ìš°ê¸°
        for sub in sub_metas:
            sub_df = pd.read_parquet(sub['path'])
            # combine_first: mainì˜ ê²°ì¸¡ì¹˜ë¥¼ subì˜ ê°’ìœ¼ë¡œ ì±„ì›€ (ì¸ë±ìŠ¤ í•©ì§‘í•©)
            main_df = main_df.combine_first(sub_df)
            
        # 3. Gold ì €ì¥
        save_path = output_dir / f"{main_meta['ticker']}.parquet"
        main_df.to_parquet(save_path)
        return True
    except Exception as e:
        print(f"    âŒ ë³‘í•© ì‹¤íŒ¨ ({main_meta['ticker']}): {e}")
        return False

def main():
    print(">>> [Phase 4] Gold Layer ìƒì„± (Correlation Based Stitching)")
    
    # 1. í´ë” ì´ˆê¸°í™”
    if GOLD_DIR.exists():
        shutil.rmtree(GOLD_DIR)
    GOLD_DIR.mkdir(parents=True, exist_ok=True)
    
    silver_files = list(SILVER_DIR.glob("*.parquet"))
    print(f"  ğŸ“– Silver íŒŒì¼ ìŠ¤ìº”: {len(silver_files)} ê°œ")

    # 2. 1ì°¨ ê·¸ë£¹í•‘ (Start Date Bucketing)
    # { '2012-05': [meta1, meta2, ...], ... }
    buckets = defaultdict(list)
    
    for f in tqdm(silver_files, desc="1. Grouping by Start Date"):
        meta = get_metadata(f)
        if meta:
            buckets[meta['start_key']].append(meta)

    # 3. ê·¸ë£¹ë³„ Correlation ê²€ì‚¬ ë° ë³‘í•©
    processed_tickers = set()
    dedup_count = 0
    merged_files_count = 0
    
    print("  ğŸ” 2. ì •ë°€ ë¶„ì„ (Correlation) & ë³‘í•© (Stitching)...")
    
    # ì§„í–‰ìƒí™© í‘œì‹œë¥¼ ìœ„í•´ ë²„í‚· ìˆœíšŒ
    for start_key, candidates in tqdm(buckets.items(), desc="Processing Buckets"):
        if len(candidates) == 1:
            # ë¹„êµ ëŒ€ìƒ ì—†ìŒ -> ë°”ë¡œ ì´ê´€
            meta = candidates[0]
            shutil.copy2(meta['path'], GOLD_DIR / f"{meta['ticker']}.parquet")
            merged_files_count += 1
            continue
            
        # ê·¸ë£¹ ë‚´ì—ì„œ ì¤‘ë³µ ì°¾ê¸°
        # ë°ì´í„°ê°€ ë§ì€(ìµœì‹ /ê¸´) ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ 'Main' í›„ë³´ ì„ ì •
        # ê¸°ì¤€: 1. ì¢…ë£Œì¼(ìµœì‹ ) 2. ë°ì´í„°ê°œìˆ˜(ê¸´ê²ƒ)
        candidates.sort(key=lambda x: (x['end_date'], x['count']), reverse=True)
        
        # ë°©ë¬¸ ì²´í¬ìš© (ê·¸ë£¹ ë‚´ ë¡œì»¬)
        local_processed = set()
        
        for i in range(len(candidates)):
            main_cand = candidates[i]
            if main_cand['ticker'] in local_processed:
                continue
                
            duplicates = []
            
            # ë‚˜ë³´ë‹¤ ë°ì´í„°ê°€ ì ê±°ë‚˜ ì˜¤ë˜ëœ ë†ˆë“¤ê³¼ ë¹„êµ
            for j in range(i + 1, len(candidates)):
                sub_cand = candidates[j]
                if sub_cand['ticker'] in local_processed:
                    continue
                
                # Correlation ê³„ì‚°
                corr = calculate_correlation(main_cand['path'], sub_cand['path'])
                
                if corr > 0.99: # 99% ì´ìƒ ì¼ì¹˜í•˜ë©´ ë™ì¼ ì¢…ëª© ê°„ì£¼
                    duplicates.append(sub_cand)
                    local_processed.add(sub_cand['ticker'])
                    dedup_count += 1
                    # ë¡œê·¸ ì¶œë ¥ (í™•ì¸ìš©)
                    # print(f"    ğŸ”— ì¤‘ë³µ ë°œê²¬: {main_cand['ticker']} == {sub_cand['ticker']} (Corr: {corr:.4f})")
            
            # ë³‘í•© ë° ì €ì¥
            if duplicates:
                stitch_and_save(main_cand, duplicates, GOLD_DIR)
            else:
                # ì¤‘ë³µ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë³µì‚¬
                shutil.copy2(main_cand['path'], GOLD_DIR / f"{main_cand['ticker']}.parquet")
            
            local_processed.add(main_cand['ticker'])
            merged_files_count += 1

    print("\n" + "="*40)
    print("  âœ… Gold Layer ìƒì„± ì™„ë£Œ")
    print(f"  - ì›ë³¸(Silver): {len(silver_files)} ê°œ")
    print(f"  - ì¤‘ë³µ ë³‘í•©ë¨(Dedup): {dedup_count} ê±´")
    print(f"  - ìµœì¢… Gold íŒŒì¼: {len(list(GOLD_DIR.glob('*.parquet')))} ê°œ")
    print("="*40)
    
    # ì¤‘ë³µ ì œê±° ë¦¬í¬íŠ¸ (ì˜µì…˜)
    if dedup_count > 0:
        print(f"  ğŸ’¡ {dedup_count}ê°œì˜ ê³¼ê±° í‹°ì»¤(FB ë“±)ê°€ ìµœì‹  í‹°ì»¤(META ë“±)ë¡œ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print("ğŸ‘‰ ì´ì œ ë°ì´í„°ëŠ” ë¬¼ë¦¬ì (Phase 3)ìœ¼ë¡œë‚˜ ë…¼ë¦¬ì (Phase 4)ìœ¼ë¡œ ì™„ë²½í•©ë‹ˆë‹¤.")
    print("ğŸ‘‰ 'Platinum Layer (Feature Engineering)' ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ì‹­ì‹œì˜¤.")

if __name__ == "__main__":
    main()