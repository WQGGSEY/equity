import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
import gc
from .cache import CacheManager
import pyarrow.parquet as pq

class MarketData:
    """
    [Robust Matrix Loader - Final Fixed Version]
    - Bias-Free: ì „ì²´ ìœ ë‹ˆë²„ìŠ¤ ë¡œë“œ
    - Auto-Universe: ê±°ë˜ëŒ€ê¸ˆ ê¸°ì¤€ Dynamic Universe Mask ìë™ ìƒì„±
    - Strict Alignment: (Dates x Tickers) í˜•íƒœ ë³´ì¥
    """
    def __init__(self, platinum_dir="data/platinum"):
        self.platinum_dir = Path(platinum_dir)
        self.prices = {}   
        self.features = {} 
        self.tickers = []
        self.dates = []
        self.cache_manager = CacheManager()
        
    def load_all(self, required_features=None):
        # ---------------------------------------------------------
        # 1. Base Data (Prices) ë¡œë”© - ìºì‹œ ìš°ì„ 
        # ---------------------------------------------------------
        # ìºì‹œ ì´ë¦„ì— 'v2'ë¥¼ ë¶™ì—¬ì„œ ê¸°ì¡´ ìºì‹œ(ìœ ë‹ˆë²„ìŠ¤ ë§ˆìŠ¤í¬ ì—†ëŠ” ë²„ì „)ì™€ ì¶©ëŒ ë°©ì§€
        base_cache_name = "market_data_base_full_universe" 
        base_data = self.cache_manager.load(base_cache_name, expiration_hours=24)
        
        if base_data:
            print(f"ğŸš€ [Loader] Base Cache Hit! Using {len(base_data['tickers'])} tickers.")
            self.prices = base_data['prices']
            self.tickers = base_data['tickers']
            self.dates = base_data['dates']
        else:
            print("ğŸš€ [Loader] Building Base Matrix (Full Universe)...")
            files = list(self.platinum_dir.glob("*.parquet"))
            if not files:
                raise FileNotFoundError(f"No parquet files in {self.platinum_dir}")

            # [Step 1] ì „ì²´ ìœ ë‹ˆë²„ìŠ¤ ìŠ¤ìº”
            print("  ğŸ§© Scanning All Files (No Limit)...")
            all_dates = set()
            all_tickers = []
            
            for p in tqdm(files, desc="  Indexing"):
                try:
                    pf = pq.ParquetFile(p)
                    if 'Close' in pf.schema.names:
                        df = pd.read_parquet(p, columns=['Close'])
                        all_dates.update(df.index)
                        all_tickers.append(p.stem)
                except:
                    continue
            
            self.dates = sorted(list(all_dates))
            self.tickers = sorted(all_tickers)
            print(f"  âœ… Universe Locked: {len(self.tickers)} tickers, {len(self.dates)} days")

            # [Step 2] ê°€ê²© ë°ì´í„° ë¡œë“œ & ì •ë ¬
            price_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
            data_store = {c: {} for c in price_cols}
            
            print(f"  ğŸ“¥ Loading Prices for {len(self.tickers)} tickers...")
            for t in tqdm(self.tickers, desc="  Reading Prices"):
                p = self.platinum_dir / f"{t}.parquet"
                try:
                    df = pd.read_parquet(p, columns=price_cols)
                    for c in price_cols:
                        if c in df.columns:
                            data_store[c][t] = df[c].astype('float32')
                except:
                    continue
            
            print("  ğŸ“ Aligning Base Matrices...")
            for c in price_cols:
                if data_store[c]:
                    df = pd.DataFrame(data_store[c])
                    # Index=Dates, Columns=Tickers
                    df = df.reindex(index=self.dates, columns=self.tickers)
                    self.prices[c] = df.astype('float32')
                del data_store[c]

            # Amount ìƒì„±
            if 'Close' in self.prices and 'Volume' in self.prices:
                open_p = self.prices.get('Open', self.prices['Close'])
                self.prices['Amount'] = ((open_p + self.prices['Close']) / 2.0 * self.prices['Volume']).astype('float32')

            # [Step 3] Dynamic Universe Mask ìƒì„± (ì—¬ê¸°ê°€ í•µì‹¬!)
            # ---------------------------------------------------------
            if 'Amount' in self.prices:
                print("  ğŸŒŒ Generating Dynamic Universe Mask (Top 500)...")
                amt = self.prices['Amount']
                
                # 1. ìµœê·¼ 20ì¼ í‰ê·  ê±°ë˜ëŒ€ê¸ˆ (Time-series Rolling)
                # axis=0 (ê¸°ë³¸ê°’)ì´ index(ë‚ ì§œ) ë°©í–¥ rolling
                rolling_amt = amt.rolling(window=20, min_periods=1).mean()
                
                # 2. ì¼ë³„ ë­í‚¹ ì‚°ì¶œ (Cross-sectional Rank)
                # axis=1 (ì»¬ëŸ¼=ì¢…ëª©) ë°©í–¥ ë­í‚¹. í° ê²Œ 1ë“±(ascending=False)
                # method='min' -> ë™ì ì ì²˜ë¦¬
                daily_rank = rolling_amt.rank(axis=1, ascending=False, method='min')
                
                # 3. ë§ˆìŠ¤í¬ ìƒì„± (Top 500ì€ 1.0, ë‚˜ë¨¸ì§€ëŠ” NaN)
                # universeì— í¬í•¨ë˜ì§€ ì•ŠëŠ” ì¢…ëª©ì„ NaNìœ¼ë¡œ ë§Œë“¤ë©´ 
                # ì „ëµì—ì„œ ê³±í•˜ê¸° ì—°ì‚° ì‹œ ìë™ìœ¼ë¡œ ì‹ í˜¸ê°€ ì£½ìŒ(NaN)
                universe_mask = daily_rank.where(daily_rank <= 500, np.nan)
                universe_mask = universe_mask.where(universe_mask.isna(), 1.0)
                
                self.prices['universe'] = universe_mask.astype('float32')
            else:
                print("  âš ï¸ Warning: Could not calculate Amount, skipping Universe generation.")

            # Base ìºì‹œ ì €ì¥
            base_save = {
                'prices': self.prices,
                'tickers': self.tickers,
                'dates': self.dates
            }
            self.cache_manager.save(base_save, base_cache_name)
            gc.collect()

        # ---------------------------------------------------------
        # 2. Feature Data ë¡œë”© (On-Demand)
        # ---------------------------------------------------------
        if required_features:
            print(f"  ğŸ“¥ Loading Features: {required_features}")
            feat_store = {f: {} for f in required_features}
            target_paths = [self.platinum_dir / f"{t}.parquet" for t in self.tickers]
            
            for p in tqdm(target_paths, desc="  Reading Features"):
                if not p.exists(): continue
                t = p.stem
                try:
                    pf = pq.ParquetFile(p)
                    file_cols = set(pf.schema.names)
                    col_map = {c.lower(): c for c in file_cols}
                    
                    read_map = {}
                    for req in required_features:
                        if req in file_cols:
                            read_map[req] = req
                        elif req.lower() in col_map:
                            read_map[col_map[req.lower()]] = req
                            
                    if not read_map: continue
                    
                    df = pd.read_parquet(p, columns=list(read_map.keys()))
                    df.rename(columns=read_map, inplace=True)
                    
                    for req in required_features:
                        if req in df.columns:
                            feat_store[req][t] = df[req].astype('float32')
                except:
                    continue
            
            for f in required_features:
                if feat_store[f]:
                    df = pd.DataFrame(feat_store[f])
                    df = df.reindex(index=self.dates, columns=self.tickers)
                    self.features[f] = df.astype('float32')
                else:
                    print(f"  âš ï¸ Feature '{f}' not found. Creating NaN matrix.")
                    self.features[f] = pd.DataFrame(np.nan, index=self.dates, columns=self.tickers).astype('float32')
                
                del feat_store[f]
            
            gc.collect()
            
        print("  âœ… Loading Complete.")