import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
import gc
import pyarrow.parquet as pq
# [FIX] CACHE_DIR ì§ì ‘ Import (data/cache ê²½ë¡œ ë³´ì¥)
from .cache import CacheManager, CACHE_DIR 

class MarketData:
    """
    [Robust Matrix Loader - Clean Cache Version]
    - Data Path: 'data/platinum/features' (Parquet Read)
    - Cache Path: 'data/cache' (Unified Cache Storage) -> Platinum í´ë” ì˜¤ì—¼ ë°©ì§€
    """
    def __init__(self, platinum_dir="data/platinum/features"):
        self.platinum_dir = Path(platinum_dir)
        
        # [FIX] Platinum ê²½ë¡œì™€ ë¬´ê´€í•˜ê²Œ ê³ ì •ëœ data/cache ê²½ë¡œ ì‚¬ìš©
        self.root_cache_dir = CACHE_DIR
        self.feature_cache_dir = self.root_cache_dir / "features"
        
        # í´ë” ìƒì„± (data/cache/features)
        self.feature_cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.prices = {}   
        self.features = {} 
        self.tickers = []
        self.dates = []
        
        # CacheManagerë„ data/cache ì‚¬ìš©
        self.cache_manager = CacheManager(cache_dir=self.root_cache_dir)
        
    def load_all(self, required_features=None):
        # ---------------------------------------------------------
        # 1. Base Data (Prices) ë¡œë”©
        # ---------------------------------------------------------
        base_cache_name = "market_data_base" 
        base_data = self.cache_manager.load(base_cache_name, expiration_hours=24)
        
        if base_data:
            print(f"ğŸš€ [Loader] Base Cache Hit! Using {len(base_data['tickers'])} tickers.")
            self.prices = base_data['prices']
            self.tickers = base_data['tickers']
            self.dates = base_data['dates']
        else:
            print("ğŸš€ [Loader] Building Base Matrix (Full Universe)...")
            files = list(self.platinum_dir.glob("*.parquet"))
            if not files:
                # features í´ë”ê°€ ì•„ë‹ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ìƒìœ„ í´ë”ë„ ê²€ìƒ‰ (í˜¸í™˜ì„±)
                files = list(self.platinum_dir.parent.glob("features/*.parquet"))
                if not files:
                    raise FileNotFoundError(f"No parquet files found in {self.platinum_dir}")

            # [Step 1] ì „ì²´ ìœ ë‹ˆë²„ìŠ¤ ì¸ë±ì‹±
            print("  ğŸ§© Scanning All Files (Indexing)...")
            all_dates = set()
            all_tickers = []
            for p in tqdm(files, desc="  Indexing"):
                try:
                    pf = pq.ParquetFile(p)
                    # ìŠ¤í‚¤ë§ˆì— Closeê°€ ìˆëŠ”ì§€ í™•ì¸
                    if 'Close' in pf.schema.names:
                        df = pd.read_parquet(p, columns=['Close'])
                        all_dates.update(df.index)
                        all_tickers.append(p.stem)
                except: continue
            
            self.dates = sorted(list(all_dates))
            self.tickers = sorted(all_tickers)
            
            # [Step 2] ê°€ê²© ë°ì´í„° ë¡œë“œ
            price_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
            data_store = {c: {} for c in price_cols}
            
            print(f"  ğŸ“¥ Loading Prices for {len(self.tickers)} tickers...")
            for t in tqdm(self.tickers, desc="  Reading Prices"):
                # íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
                p = self.platinum_dir / f"{t}.parquet"
                if not p.exists(): continue
                
                try:
                    df = pd.read_parquet(p, columns=price_cols)
                    for c in price_cols:
                        if c in df.columns: data_store[c][t] = df[c].astype('float32')
                except: continue
            
            # ì •ë ¬ ë° ê²°ì¸¡ì¹˜ ë³´ê°„
            print("  ğŸ“ Aligning Base Matrices...")
            for c in price_cols:
                if data_store[c]:
                    df = pd.DataFrame(data_store[c])
                    df = df.reindex(index=self.dates, columns=self.tickers)
                    self.prices[c] = df.astype('float32').ffill() # ffill ì ìš©
                del data_store[c]

            # Amount & Universe ìƒì„±
            if 'Close' in self.prices and 'Volume' in self.prices:
                open_p = self.prices.get('Open', self.prices['Close'])
                self.prices['Amount'] = ((open_p + self.prices['Close']) / 2.0 * self.prices['Volume']).astype('float32')
                
                # Universe Mask (ë™ì „ì£¼ í•„í„° ì ìš©)
                print("  ğŸŒŒ Generating Dynamic Universe Mask (Top 500, Price > $1)...")
                amt = self.prices['Amount']
                close = self.prices['Close']
                valid_price_mask = (close > 1.0)
                rolling_amt = amt.rolling(window=20, min_periods=1).mean()
                filtered_amt = rolling_amt.where(valid_price_mask, 0.0)
                daily_rank = filtered_amt.rank(axis=1, ascending=False, method='min')
                universe_mask = daily_rank.where(daily_rank <= 500, np.nan)
                self.prices['universe'] = universe_mask.where(universe_mask.isna(), 1.0).astype('float32')

            # Base ìºì‹œ ì €ì¥ (data/cache/...)
            base_save = {'prices': self.prices, 'tickers': self.tickers, 'dates': self.dates}
            self.cache_manager.save(base_save, base_cache_name)
            gc.collect()

        # ---------------------------------------------------------
        # 2. Modular Feature Loading (ê°œë³„ ìºì‹± ì ìš©)
        # ---------------------------------------------------------
        if required_features:
            self._load_features_modular(required_features)
            
        print("  âœ… Loading Complete.")

    def _load_features_modular(self, required_features):
        """
        í•„ìš”í•œ í”¼ì²˜ë§Œ ê³¨ë¼ì„œ ë¡œë“œí•˜ê³ , ì—†ëŠ” ê²ƒë§Œ íŒŒì¼ì—ì„œ ì¶”ì¶œí•˜ì—¬ 'data/cache/features'ì— ì €ì¥í•¨.
        """
        missing_features = []
        
        # 1. ê¸°ì¡´ ìºì‹œ í™•ì¸ ë° ë¡œë“œ
        print(f"  ğŸ” Checking Feature Caches: {required_features}")
        for feat in required_features:
            # [FIX] Feature ìºì‹œë„ data/cache/features ì—ì„œ ì°¾ìŒ
            cache_path = self.feature_cache_dir / f"{feat}.parquet"
            if cache_path.exists():
                try:
                    self.features[feat] = pd.read_parquet(cache_path)
                    # ì¸ë±ìŠ¤ ì •í•©ì„± ì²´í¬
                    if not self.features[feat].index.equals(pd.Index(self.dates)):
                        print(f"    âš ï¸ Cache mismatch for {feat}. Re-queuing.")
                        missing_features.append(feat)
                except:
                    missing_features.append(feat)
            else:
                missing_features.append(feat)
                
        if not missing_features:
            print("    -> All features loaded from cache!")
            return

        # 2. ì—†ëŠ” í”¼ì²˜(Missing)ë§Œ íŒŒì¼ì—ì„œ ì¶”ì¶œ
        print(f"  ğŸ“¥ Extracting Missing Features: {missing_features}")
        feat_store = {f: {} for f in missing_features}
        
        # íŒŒì¼ ìŠ¤ìº”
        for t in tqdm(self.tickers, desc="  Scanning Files"):
            p = self.platinum_dir / f"{t}.parquet"
            if not p.exists(): continue
            try:
                pf = pq.ParquetFile(p)
                file_cols = set(pf.schema.names)
                
                read_map = {}
                col_map_lower = {c.lower(): c for c in file_cols}
                
                for req in missing_features:
                    if req in file_cols:
                        read_map[req] = req
                    elif req.lower() in col_map_lower:
                        read_map[col_map_lower[req.lower()]] = req
                
                if not read_map: continue
                
                df = pd.read_parquet(p, columns=list(read_map.keys()))
                df.rename(columns=read_map, inplace=True)
                
                for req in missing_features:
                    if req in df.columns:
                        feat_store[req][t] = df[req].astype('float32')
            except:
                continue
        
        # 3. DataFrame ë³€í™˜ ë° ì €ì¥
        print("  ğŸ’¾ Caching New Features to data/cache/features...")
        for f in missing_features:
            if feat_store[f]:
                df = pd.DataFrame(feat_store[f])
                df = df.reindex(index=self.dates, columns=self.tickers).astype('float32')
                
                self.features[f] = df
                
                # [FIX] data/cache/features ì— ì €ì¥
                save_path = self.feature_cache_dir / f"{f}.parquet"
                df.to_parquet(save_path)
            else:
                print(f"    âš ï¸ Feature '{f}' not found in any file. Filling with NaN.")
                df = pd.DataFrame(np.nan, index=self.dates, columns=self.tickers).astype('float32')
                self.features[f] = df
                df.to_parquet(self.feature_cache_dir / f"{f}.parquet")
            
            del feat_store[f]
        
        gc.collect()