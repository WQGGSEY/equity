import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
import gc
from .cache import CacheManager
import pyarrow.parquet as pq

class MarketData:
    """
    [Robust Matrix Loader - Final Fixed Version]
    - Bias-Free: ì¸ìœ„ì ì¸ ì¢…ëª© ìˆ˜ ì œí•œ(3000ê°œ) ì—†ì´ ì „ì²´ ìœ ë‹ˆë²„ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    - Efficient Caching: ë³€í•˜ì§€ ì•ŠëŠ” 'ê°€ê²©(Base)'ê³¼ ë³€í•˜ëŠ” 'í”¼ì²˜(Feature)'ë¥¼ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    - Strict Alignment: ëª¨ë“  í–‰ë ¬ì´ (Dates x Tickers) í˜•íƒœë¥¼ ê°–ë„ë¡ ê°•ì œí•˜ì—¬ ì—°ì‚° ì˜¤ë¥˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    def __init__(self, platinum_dir="data/platinum"):
        self.platinum_dir = Path(platinum_dir)
        self.prices = {}   
        self.features = {} 
        self.tickers = []
        self.dates = []
        self.cache_manager = CacheManager()
        
    def load_all(self, required_features=None):
        # ---------------------------------------------------------
        # 1. Base Data (Prices) ë¡œë”© - ìºì‹œ ìš°ì„ 
        # ---------------------------------------------------------
        # í”¼ì²˜ê°€ ë°”ë€Œì–´ë„ ê°€ê²© ë°ì´í„° ìºì‹œëŠ” ê·¸ëŒ€ë¡œ ì”ë‹ˆë‹¤. (ë¹„íš¨ìœ¨ ì œê±°)
        base_cache_name = "market_data_base_full_universe"
        base_data = self.cache_manager.load(base_cache_name, expiration_hours=24)
        
        if base_data:
            print(f"ğŸš€ [Loader] Base Cache Hit! Using {len(base_data['tickers'])} tickers.")
            self.prices = base_data['prices']
            self.tickers = base_data['tickers']
            self.dates = base_data['dates']
        else:
            print("ğŸš€ [Loader] Building Base Matrix (Full Universe)...")
            files = list(self.platinum_dir.glob("*.parquet"))
            if not files:
                raise FileNotFoundError(f"No parquet files in {self.platinum_dir}")

            # [Step 1] ì „ì²´ ìœ ë‹ˆë²„ìŠ¤ ìŠ¤ìº” (Bias-Free)
            print("  ğŸ§© Scanning All Files (No Limit)...")
            all_dates = set()
            all_tickers = []
            
            # 13,000ê°œ íŒŒì¼ ìŠ¤ìº” (ë‚ ì§œì¶• í™•ì •ìš©)
            for p in tqdm(files, desc="  Indexing"):
                try:
                    pf = pq.ParquetFile(p)
                    # Close ì»¬ëŸ¼ì´ ìˆëŠ” íŒŒì¼ë§Œ ìœ íš¨í•œ ì¢…ëª©ìœ¼ë¡œ ì¸ì •
                    if 'Close' in pf.schema.names:
                        # ë‚ ì§œ ì¸ë±ìŠ¤ë§Œ ë¹ ë¥´ê²Œ ì¶”ì¶œ
                        df = pd.read_parquet(p, columns=['Close'])
                        all_dates.update(df.index)
                        all_tickers.append(p.stem)
                except:
                    continue
            
            self.dates = sorted(list(all_dates))
            self.tickers = sorted(all_tickers)
            print(f"  âœ… Universe Locked: {len(self.tickers)} tickers, {len(self.dates)} days")

            # [Step 2] ê°€ê²© ë°ì´í„° ë¡œë“œ & ì •ë ¬
            price_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
            data_store = {c: {} for c in price_cols}
            
            print(f"  ğŸ“¥ Loading Prices for {len(self.tickers)} tickers...")
            for t in tqdm(self.tickers, desc="  Reading Prices"):
                p = self.platinum_dir / f"{t}.parquet"
                try:
                    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì½ê¸°
                    df = pd.read_parquet(p, columns=price_cols)
                    for c in price_cols:
                        if c in df.columns:
                            # float32ë¡œ ë³€í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½ (13,000ê°œ ë¡œë“œ í•„ìˆ˜ ì¡°ê±´)
                            data_store[c][t] = df[c].astype('float32')
                except:
                    continue
            
            # ë§¤íŠ¸ë¦­ìŠ¤ ë³€í™˜ (Strict Alignment)
            print("  ğŸ“ Aligning Base Matrices...")
            for c in price_cols:
                if data_store[c]:
                    df = pd.DataFrame(data_store[c])
                    # [í•µì‹¬] ê¸°ì¤€ Dateì™€ Tickerë¡œ ê°•ì œ ì¬ì •ë ¬ (ë¹ˆ ê³³ì€ NaN)
                    # Engineì€ (Date x Ticker)ë¥¼ ì›í•˜ë¯€ë¡œ Transpose ì•ˆ í•¨ (BacktestEngine.run ë¡œì§ ê¸°ì¤€)
                    # ë§Œì•½ Engineì´ Transposeë¥¼ ì›í•˜ë©´ self.prices[c] = df.reindex(...).T ë¡œ ë³€ê²½í•´ì•¼ í•¨
                    # ì—¬ê¸°ì„œëŠ” ì‚¬ìš©ì ì½”ë“œê°€ pandas DataFrame(Date index)ë¥¼ ì›í•œë‹¤ê³  ê°€ì •
                    df = df.reindex(index=self.dates, columns=self.tickers)
                    self.prices[c] = df.astype('float32')
                del data_store[c]

            # Amount ìƒì„±
            if 'Close' in self.prices and 'Volume' in self.prices:
                open_p = self.prices.get('Open', self.prices['Close'])
                self.prices['Amount'] = ((open_p + self.prices['Close']) / 2.0 * self.prices['Volume']).astype('float32')

            # Base ìºì‹œ ì €ì¥
            base_save = {
                'prices': self.prices,
                'tickers': self.tickers,
                'dates': self.dates
            }
            self.cache_manager.save(base_save, base_cache_name)
            gc.collect()

        # ---------------------------------------------------------
        # 2. Feature Data ë¡œë”© (On-Demand from Disk)
        # ---------------------------------------------------------
        # í”¼ì²˜ëŠ” ìºì‹œí•˜ì§€ ì•Šê³ , ì´ë¯¸ í™•ë³´ëœ self.tickersë¥¼ ì´ìš©í•´ í•„ìš”í•œ ê²ƒë§Œ ë¹ ë¥´ê²Œ ì½ìŠµë‹ˆë‹¤.
        # ì´ë ‡ê²Œ í•˜ë©´ "í”¼ì²˜ ë°”ë€” ë•Œë§ˆë‹¤ ìºì‹œ ë‹¤ì‹œ ë§Œë“œëŠ”" ë¬¸ì œê°€ í•´ê²°ë©ë‹ˆë‹¤.
        
        if required_features:
            print(f"  ğŸ“¥ Loading Features: {required_features}")
            
            # í”¼ì²˜ë³„ ì„ì‹œ ì €ì¥ì†Œ
            feat_store = {f: {} for f in required_features}
            
            # ì´ë¯¸ í™•ë³´ëœ ìœ ë‹ˆë²„ìŠ¤(self.tickers)ì— ëŒ€í•´ì„œë§Œ íŒŒì¼ì„ ì—½ë‹ˆë‹¤.
            # (ì „ì²´ ë””ë ‰í† ë¦¬ ìŠ¤ìº” X -> ì†ë„ í–¥ìƒ)
            target_paths = [self.platinum_dir / f"{t}.parquet" for t in self.tickers]
            
            for p in tqdm(target_paths, desc="  Reading Features"):
                if not p.exists(): continue
                t = p.stem
                
                try:
                    # ìŠ¤í‚¤ë§ˆ í™•ì¸ (ëŒ€ì†Œë¬¸ì ë³´ì • ë° ì¡´ì¬ ì—¬ë¶€ í™•ì¸)
                    pf = pq.ParquetFile(p)
                    file_cols = set(pf.schema.names)
                    col_map = {c.lower(): c for c in file_cols}
                    
                    read_map = {} # {ì‹¤ì œì´ë¦„: ìš”ì²­ì´ë¦„}
                    for req in required_features:
                        if req in file_cols:
                            read_map[req] = req
                        elif req.lower() in col_map: # Fuzzy Match
                            read_map[col_map[req.lower()]] = req
                            
                    if not read_map: continue
                    
                    # ì½ê¸°
                    df = pd.read_parquet(p, columns=list(read_map.keys()))
                    df.rename(columns=read_map, inplace=True)
                    
                    for req in required_features:
                        if req in df.columns:
                            feat_store[req][t] = df[req].astype('float32')
                            
                except:
                    continue
            
            # ë§¤íŠ¸ë¦­ìŠ¤ ë³€í™˜ ë° ì •ë ¬
            for f in required_features:
                if feat_store[f]:
                    df = pd.DataFrame(feat_store[f])
                    # Baseì™€ ë™ì¼í•œ Shape ê°•ì œ
                    df = df.reindex(index=self.dates, columns=self.tickers)
                    self.features[f] = df.astype('float32')
                else:
                    print(f"  âš ï¸ Feature '{f}' not found. Creating NaN matrix.")
                    self.features[f] = pd.DataFrame(np.nan, index=self.dates, columns=self.tickers).astype('float32')
                
                del feat_store[f]
            
            gc.collect()
            
        print("  âœ… Loading Complete.")